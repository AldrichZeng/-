#### 1、LeNet-5 on MNIST-10
|层  |剪枝率|首次测试|首次迭代|acc|Epoch|学习率|学习率衰减|batch|
|:---|:-----|:-------|:------|:-------|:------|:-----|:--------|:---------|
|    |0%    |/       |/      |0.9902  |185    | 0.001|无       |256|
|Conv|50%   |0.9861|0.9883 |0.9898  |154    |0.001 |无       |256|
|Conv|70%   |0.9833|0.9878 |0.9900  |261    |0.001 |0.1/50epoch|128|
|Conv|90%   |0.3572|0.9678 |0.9884  |206    |0.001 |0.1/50epoch|128|
|FC  |50%   |0.9832|0.9878 |0.9893  |287    |0.001 |0.1/50epoch|128|
|FC  |70%   |0.9619|0.9824 |0.9881  |273    |0.001 |0.1/50epoch|128|
|FC  |90%   |0.8973  |0.9767 |0.9865  |166    |0.001 |0.1/50epoch|128|

#### 2、AlextNet on CIFAR-10
考虑到梯度下降的收敛速度，采用Adam优化器：
`betas=(0.9, 0.999), eps=1e-08, weight_decay=0`

|层  |剪枝率|首次测试|首次迭代|acc|Epoch|学习率|学习率衰减|batch|备注|
|:---|:-----|:------|:--------|:-------|:------|:-----|:--------|:---------|:---|
|    |0%    |/      |         |0.75474 |636    | 0.001|无|128||
|Conv|50%   |0.3965 |0.4386   |0.6121  |520    |0.001 |无|128||
|Conv|90%   |0.2843 |0.4579   |0.6284  |174    |0.001 |0.1/50epoch|128|保留Conv1|
|FC  |50%   |0.6831 |0.6873   |0.7245  |283    |0.001 |0.1/50epoch|128||
|FC  |90%   |0.6857 |0.7080   |0.7508  |458    |0.001 |0.1/100epoch|128||

#### 3、VGG-16_bn on CIFAR-10
|层  |剪枝率|首次测试|首次迭代|acc|Epoch|学习率|学习率衰减|batch|备注|
|:---|:-----|:------|:------|:--|:------|:-----|:--------|:---------|:--|
|    |0%    |/     |   |  |       |      |         |16||
|Conv|50%   |0.0003 |0.2671 |   |     |0.001 |0.1/50epoch|16|   |
|Conv|90%   |      |   |  |     |       |          |16|     |
|FC|50%   |      |   |  |     |       |          |16|     |
|FC|90%   |      |   |  |     |       |          |16|     |
#### 参考论文
1. 《Deep compression - Compressing Deep Neural Networks With Pruning, Tranied Quantization And Huffman Coding》
2. 《Leaning both Weights and Connections for Efficient Neural Networks》

————2019年4月28日
